### Exercise 1.7

The `is_good_enough` test used in computing square roots is not very effective for finding the square roots of **very small numbers**. Also, in real computers, arithmetic operations are almost always performed with **limited precision**. This makes our test inadequate for **very large numbers** as well.

Explain these statements, with examples showing how the test fails for small and large numbers.

An alternative strategy for implementing `is_good_enough` is to watch how the **guess changes from one iteration to the next** and to stop when the change is a very small **fraction of the guess**.

Design a square-root function that uses this kind of end test. Does this work better for small and large numbers?

---

### Answer

#### 1. Why the Original `is_good_enough` Test Fails for Very Small Numbers

The original test checks whether the absolute difference between `guess²` and `x` is smaller than `0.001`:

```
|guess² - x| < 0.001
```

This test fails for **very small numbers** because `0.001` is a **huge threshold** when compared to tiny values.

#### Example

Suppose we want to compute:

```
sqrt(0.0001)
```

The correct answer is:

```
0.01
```

Now assume the iterative guess becomes:

```
guess = 0.03
```

Then:

```
guess² = 0.0009
|0.0009 - 0.0001| = 0.0008
```

Since:

```
0.0008 < 0.001
```

the program incorrectly decides that the answer is **good enough** and stops, even though `0.03` is **three times larger than the correct value `0.01`**.

#### Conclusion

When the number itself is smaller than the fixed threshold (`0.001`), the program **stops too early**, producing a **poor result**.

---

#### 2. Why the Test Also Fails for Very Large Numbers

As stated in the question, real computers use **limited precision** to represent numbers. For extremely **large values**, squaring a large `guess` produces numbers so big that the computer:

* Cannot track **tiny differences accurately**
* Must **round off** small decimal values

#### Example

Consider a guess like:

```
1334343443.2342
```

Squaring this value results in a massive number where:

* Small decimal changes are **lost due to rounding**
* The expression `|guess² - x|` may:

  * Become `0` too early
  * Never become smaller than `0.001`
  * Or behave **unpredictably**

This makes the test:

```
|guess² - x| < 0.001
```

**unreliable** for very large numbers.

#### Important Clarification

The problem is **not merely that large numbers are inaccurate**. The real issue is that **floating-point numbers lose the ability to represent very small differences at large scales**.

---

#### 3. Improved `is_good_enough` Strategy (Change-Based Test)

Instead of checking how close `guess²` is to `x`, a better approach is to check:

> **How much the guess itself changes between iterations**

#### Key Idea

* Stop when the **new guess is almost the same as the old guess**
* The difference must be a **fraction of the guess**, not a fixed number

#### Example

Let the threshold be **1% of the old guess**.

If:

```
old_guess = 10
new_guess = 9.1
threshold = 10 × 0.01 = 0.1
```

Then:

```
|10 - 9.1| = 0.9 > 0.1   Not close enough
```

But if:

```
old_guess = 10
new_guess = 10.02
```

Then:

```
|10 - 10.02| = 0.02 < 0.1   Close enough, stop
```

#### In Simple Words

> Stop when the change between two consecutive guesses is **very small compared to the size of the current guess**.

---

#### 4. Why This Method Works Better

This improved method:

* Does **not stop early** for small numbers
* Does **not loop endlessly** for large numbers
* Automatically **adapts to the scale of the number**
* Uses a **dynamic threshold** instead of a fixed constant

PS: 

One might think like I did that , we can just impore it by old_guess  -  new_guess < .001 , which means it should be accurate to 3 decimal places and think that 0.001 means the same for all numbers but if we take a example 

* For 0.01, an error of 0.001 = 10% error
* For 3000, an error of 0.001 = 0.000033% error

By this we understand how flawed our static logic was.

